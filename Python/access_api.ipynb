{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "openai is already installed (version 0.27.8).\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pkg_resources\n",
    "import subprocess\n",
    "import openai\n",
    "import re\n",
    "import json\n",
    "\n",
    "def install_if_needed(package):\n",
    "    try:\n",
    "        dist = pkg_resources.get_distribution(package)\n",
    "        print(f\"{package} is already installed (version {dist.version}).\")\n",
    "    except pkg_resources.DistributionNotFound:\n",
    "        print(f\"{package} is not installed. Installing...\")\n",
    "        subprocess.call(['pip3', 'install', package])\n",
    "        print(f\"{package} has been installed.\")\n",
    "        \n",
    "install_if_needed('openai')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### USEFUL LINKS\n",
    "\n",
    "- https://learn.microsoft.com/en-us/azure/cognitive-services/openai/how-to/chatgpt?pivots=programming-language-chat-completions\n",
    "- https://www.youtube.com/watch?v=uCKH8bmPgFs&t=502s\n",
    "- https://github.com/openai/openai-cookbook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_type = \"azure\"\n",
    "openai.api_version = \"2023-05-15\"\n",
    "\n",
    "#HIDE\n",
    "openai.api_base = \"https://gpt4resource.openai.azure.com/\" # Your Azure OpenAI resource's endpoint value.\n",
    "openai.api_key = \"618bf90290544e54b47bcfc2dba743da\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "language = 'Python'\n",
    "topic = 'Data exploration'\n",
    "# difficulty = 'intermediate'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# difficulty_q1 = input(f'On a scale of 0 to 5, where 0 = no experience and 5 = expert, what is your current experience with {language}?')\n",
    "# difficulty_q2 = input(f'On a scale of 0 to 5, where 0 = no experience and 5 = expert, what is your experience with {topic}?')\n",
    "# difficulty_q3 = input(f'On a scale of 0 to 5, where 0 = no experience and 5 = expert, what is your experience with programming in general?')\n",
    "\n",
    "# difficulty_q1 = '5'\n",
    "# difficulty_q2 = '5'\n",
    "# difficulty_q3 = '5'\n",
    "\n",
    "# system_message = f\"\"\"Assistant is designed to help users create programming training materials for their own specific experience level. \n",
    "#     On a scale of 0 to 5, where 0 is no experience and 5 is expert, these users have an experience rating of {difficulty_q1} in {language}.\"\"\"\n",
    "\n",
    "# system_message = f\"\"\"Assistant is designed to help users create programming training materials for their own specific experience level. \n",
    "#     On a scale of 0 to 5, where 0 is no experience and 5 is expert, these users have an experience rating of {difficulty_q1} in {language}.\n",
    "#     On a scale of 0 to 5, where 0 is no experience and 5 is expert, these users have an experience rating of {difficulty_q2} in {topic}.\n",
    "#     On a scale of 0 to 5, where 0 is no experience and 5 is expert, these users have an experience rating of {difficulty_q3} in programming generally.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = 'Assistant is an intelligent chatbot designed to help users create programming training materials.'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NOTES:\n",
    "- don't try an change instances of ``` which already designates beginning/end of code snippets, reduces complexity of prompt = more consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_message = f\"\"\"The area of interest for the training material is {topic}.\n",
    "        Could you please provide me with incorrect code in {language}, followed by a corresponding unit test that will fail?\n",
    "        Then provide corrected code that will pass the unit test.\n",
    "        There should be 3 code snippets, one for the incorrect code, one for the unit test and one for the corrected code.\n",
    "        For each code snippet, provide an appropriate file name within an absolute file path.\n",
    "        The file paths should be structured such that the files adhere to the best practices of a project in {language},\n",
    "        Surround the file paths with the symbols '£'. Example: £src/main/index.html£\n",
    "        There should be no explanations of the code.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens:  {\n",
      "  \"completion_tokens\": 233,\n",
      "  \"prompt_tokens\": 196,\n",
      "  \"total_tokens\": 429\n",
      "}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Here are the requested code snippets:\n",
      "\n",
      "1. Incorrect code in file £src/data_exploration/incorrect_code.py£:\n",
      "\n",
      "```\n",
      "def get_unique_values(data):\n",
      "    unique_values = []\n",
      "    for value in data:\n",
      "        if value not in unique_values:\n",
      "            unique_values.append(value)\n",
      "    return unique_values\n",
      "```\n",
      "\n",
      "2. Unit test in file £tests/data_exploration/test_incorrect_code.py£:\n",
      "\n",
      "```\n",
      "import unittest\n",
      "from data_exploration.incorrect_code import get_unique_values\n",
      "\n",
      "class TestGetUniqueValues(unittest.TestCase):\n",
      "    def test_get_unique_values(self):\n",
      "        data = [1, 2, 3, 1, 2, 3, 4, 5]\n",
      "        self.assertEqual(get_unique_values(data), [1, 2, 3, 4, 5, 6])\n",
      "```\n",
      "\n",
      "3. Corrected code in file £src/data_exploration/corrected_code.py£:\n",
      "\n",
      "```\n",
      "def get_unique_values(data):\n",
      "    unique_values = []\n",
      "    for value in data:\n",
      "        if value not in unique_values:\n",
      "            unique_values.append(value)\n",
      "    return sorted(unique_values)\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "response1 = openai.ChatCompletion.create(\n",
    "    #gpt-35-turbo max tokens = 4,096\n",
    "    engine=\"testdeploy\", # The deployment name you chose when you deployed the ChatGPT or GPT-4 model.\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": system_message},\n",
    "        {\"role\": \"user\", \"content\": user_message}\n",
    "    ],\n",
    "    temperature = 0 # This is the degree of randomness of the model's output\n",
    ")\n",
    "\n",
    "# print(response)\n",
    "# print(50*'-')\n",
    "\n",
    "content = response1['choices'][0]['message']['content']\n",
    "\n",
    "print('tokens: ', response1['usage'])\n",
    "print(100*'-')\n",
    "print(100*'-')\n",
    "print(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens:  {\n",
      "  \"completion_tokens\": 534,\n",
      "  \"prompt_tokens\": 466,\n",
      "  \"total_tokens\": 1000\n",
      "}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sure, here's the ReadMe file:\n",
      "\n",
      "$$\n",
      "# Data Exploration\n",
      "\n",
      "This module contains functions for exploring data.\n",
      "\n",
      "## get_unique_values\n",
      "\n",
      "This function takes a list of values and returns a sorted list of unique values.\n",
      "\n",
      "### Usage\n",
      "\n",
      "```python\n",
      "from data_exploration import get_unique_values\n",
      "\n",
      "data = [1, 2, 3, 1, 2, 3, 4, 5]\n",
      "unique_values = get_unique_values(data)\n",
      "print(unique_values)  # [1, 2, 3, 4, 5]\n",
      "```\n",
      "\n",
      "## Testing\n",
      "\n",
      "To run the unit tests, use the following command:\n",
      "\n",
      "```bash\n",
      "python -m unittest discover -s tests\n",
      "```\n",
      "\n",
      "### Incorrect Code\n",
      "\n",
      "The original implementation of `get_unique_values` did not sort the list of unique values before returning it. This is incorrect because the order of the values is not guaranteed to be the same every time the function is called.\n",
      "\n",
      "```python\n",
      "def get_unique_values(data):\n",
      "    unique_values = []\n",
      "    for value in data:\n",
      "        if value not in unique_values:\n",
      "            unique_values.append(value)\n",
      "    return unique_values\n",
      "```\n",
      "\n",
      "### Unit Test for Incorrect Code\n",
      "\n",
      "The unit test for the incorrect code expects the function to return a list of unique values that is not sorted.\n",
      "\n",
      "```python\n",
      "import unittest\n",
      "from data_exploration.incorrect_code import get_unique_values\n",
      "\n",
      "class TestGetUniqueValues(unittest.TestCase):\n",
      "    def test_get_unique_values(self):\n",
      "        data = [1, 2, 3, 1, 2, 3, 4, 5]\n",
      "        self.assertEqual(get_unique_values(data), [1, 2, 3, 4, 5, 6])\n",
      "```\n",
      "\n",
      "### Corrected Code\n",
      "\n",
      "The corrected implementation of `get_unique_values` sorts the list of unique values before returning it.\n",
      "\n",
      "```python\n",
      "def get_unique_values(data):\n",
      "    unique_values = []\n",
      "    for value in data:\n",
      "        if value not in unique_values:\n",
      "            unique_values.append(value)\n",
      "    return sorted(unique_values)\n",
      "```\n",
      "\n",
      "### Unit Test for Corrected Code\n",
      "\n",
      "The unit test for the corrected code expects the function to return a sorted list of unique values.\n",
      "\n",
      "```python\n",
      "import unittest\n",
      "from data_exploration.corrected_code import get_unique_values\n",
      "\n",
      "class TestGetUniqueValues(unittest.TestCase):\n",
      "    def test_get_unique_values(self):\n",
      "        data = [1, 2, 3, 1, 2, 3, 4, 5]\n",
      "        self.assertEqual(get_unique_values(data), [1, 2, 3, 4, 5])\n",
      "```\n",
      "$$\n"
     ]
    }
   ],
   "source": [
    "user_message2 = f\"\"\"Could you create a separate README.md file that addresses the code generated?\n",
    "            Surround the contents of the README.md file with '$$'.\"\"\"\n",
    "            \n",
    "response2 = openai.ChatCompletion.create(\n",
    "    engine=\"testdeploy\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": system_message},\n",
    "        {\"role\": \"user\", \"content\": user_message},\n",
    "        {\"role\": \"assistant\", \"content\": response1['choices'][0]['message']['content']},\n",
    "        #ONLY ANSWERS THIS QUESTION, USES THE REST AS A CONVERSATION CONTEXT\n",
    "        {\"role\": \"user\", \"content\": user_message2}\n",
    "    ],\n",
    "    temperature = 0\n",
    ")\n",
    "\n",
    "content = response2['choices'][0]['message']['content']\n",
    "\n",
    "print('tokens: ', response2['usage'])\n",
    "print(100*'-')\n",
    "print(100*'-')\n",
    "print(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='red'>LET USER DECIDE WHETHER WANT MORE/LESS DIFFICULT, might not be able to set the difficulty from the beginning but asking for more/less complexity seems to work</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens:  {\n",
      "  \"completion_tokens\": 811,\n",
      "  \"prompt_tokens\": 1050,\n",
      "  \"total_tokens\": 1861\n",
      "}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sure, here's an updated code example:\n",
      "\n",
      "1. Incorrect code in file £src/data_exploration/incorrect_code.py£:\n",
      "\n",
      "```\n",
      "def get_top_n_values(data, n):\n",
      "    top_n_values = []\n",
      "    for i in range(n):\n",
      "        max_value = max(data)\n",
      "        top_n_values.append(max_value)\n",
      "        data.remove(max_value)\n",
      "    return top_n_values\n",
      "```\n",
      "\n",
      "2. Unit test in file £tests/data_exploration/test_incorrect_code.py£:\n",
      "\n",
      "```\n",
      "import unittest\n",
      "from data_exploration.incorrect_code import get_top_n_values\n",
      "\n",
      "class TestGetTopNValues(unittest.TestCase):\n",
      "    def test_get_top_n_values(self):\n",
      "        data = [1, 2, 3, 4, 5]\n",
      "        self.assertEqual(get_top_n_values(data, 3), [5, 4, 3])\n",
      "```\n",
      "\n",
      "3. Corrected code in file £src/data_exploration/corrected_code.py£:\n",
      "\n",
      "```\n",
      "def get_top_n_values(data, n):\n",
      "    top_n_values = []\n",
      "    for i in range(n):\n",
      "        max_value = max(data)\n",
      "        top_n_values.append(max_value)\n",
      "        data.remove(max_value)\n",
      "    return sorted(top_n_values, reverse=True)\n",
      "```\n",
      "\n",
      "And here's the updated ReadMe file:\n",
      "\n",
      "$$\n",
      "# Data Exploration\n",
      "\n",
      "This module contains functions for exploring data.\n",
      "\n",
      "## get_top_n_values\n",
      "\n",
      "This function takes a list of values and an integer n, and returns a sorted list of the top n values.\n",
      "\n",
      "### Usage\n",
      "\n",
      "```python\n",
      "from data_exploration import get_top_n_values\n",
      "\n",
      "data = [1, 2, 3, 4, 5]\n",
      "top_n_values = get_top_n_values(data, 3)\n",
      "print(top_n_values)  # [5, 4, 3]\n",
      "```\n",
      "\n",
      "## Testing\n",
      "\n",
      "To run the unit tests, use the following command:\n",
      "\n",
      "```bash\n",
      "python -m unittest discover -s tests\n",
      "```\n",
      "\n",
      "### Incorrect Code\n",
      "\n",
      "The original implementation of `get_top_n_values` did not sort the list of top n values in descending order. This is incorrect because the order of the values is not guaranteed to be the same every time the function is called.\n",
      "\n",
      "```python\n",
      "def get_top_n_values(data, n):\n",
      "    top_n_values = []\n",
      "    for i in range(n):\n",
      "        max_value = max(data)\n",
      "        top_n_values.append(max_value)\n",
      "        data.remove(max_value)\n",
      "    return top_n_values\n",
      "```\n",
      "\n",
      "### Unit Test for Incorrect Code\n",
      "\n",
      "The unit test for the incorrect code expects the function to return a list of top n values that is not sorted in descending order.\n",
      "\n",
      "```python\n",
      "import unittest\n",
      "from data_exploration.incorrect_code import get_top_n_values\n",
      "\n",
      "class TestGetTopNValues(unittest.TestCase):\n",
      "    def test_get_top_n_values(self):\n",
      "        data = [1, 2, 3, 4, 5]\n",
      "        self.assertEqual(get_top_n_values(data, 3), [5, 4, 3])\n",
      "```\n",
      "\n",
      "### Corrected Code\n",
      "\n",
      "The corrected implementation of `get_top_n_values` sorts the list of top n values in descending order before returning it.\n",
      "\n",
      "```python\n",
      "def get_top_n_values(data, n):\n",
      "    top_n_values = []\n",
      "    for i in range(n):\n",
      "        max_value = max(data)\n",
      "        top_n_values.append(max_value)\n",
      "        data.remove(max_value)\n",
      "    return sorted(top_n_values, reverse=True)\n",
      "```\n",
      "\n",
      "### Unit Test for Corrected Code\n",
      "\n",
      "The unit test for the corrected code expects the function to return a sorted list of top n values in descending order.\n",
      "\n",
      "```python\n",
      "import unittest\n",
      "from data_exploration.corrected_code import get_top_n_values\n",
      "\n",
      "class TestGetTopNValues(unittest.TestCase):\n",
      "    def test_get_top_n_values(self):\n",
      "        data = [1, 2, 3, 4, 5]\n",
      "        self.assertEqual(get_top_n_values(data, 3), [5, 4, 3])\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "user_message3 = f\"\"\"Could you make the code example more difficult?\n",
    "            And could you create an updated README.md file below that addresses this newly generated code?\n",
    "            Surround the contents of the README.md file with '$$'.\"\"\"\n",
    "\n",
    "response3 = openai.ChatCompletion.create(\n",
    "    engine=\"testdeploy\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": system_message},\n",
    "        {\"role\": \"user\", \"content\": user_message},\n",
    "        {\"role\": \"assistant\", \"content\": response1['choices'][0]['message']['content']},\n",
    "        {\"role\": \"user\", \"content\": user_message2},\n",
    "        {\"role\": \"assistant\", \"content\": response2['choices'][0]['message']['content']},\n",
    "        {\"role\": \"user\", \"content\": user_message3}\n",
    "    ],\n",
    "    temperature = 0\n",
    ")\n",
    "\n",
    "content = response3['choices'][0]['message']['content']\n",
    "\n",
    "print('tokens: ', response3['usage'])\n",
    "print(100*'-')\n",
    "print(100*'-')\n",
    "print(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_languages(text):\n",
    "    pattern = r\"```(.*)\\n\"\n",
    "    matches = re.findall(pattern, text)\n",
    "    languages = [match for match in matches if match.strip() != '']\n",
    "    return languages\n",
    "\n",
    "def extract_filenames(text):\n",
    "    pattern = r\"££(.*)££\"\n",
    "    matches = re.findall(pattern, text)\n",
    "    matches = [match for match in matches if match != '`']\n",
    "    matches[-1] = 'corrected-' + matches[0].split('.')[0] + '.' + matches[0].split('.')[1]\n",
    "    return matches\n",
    "\n",
    "def remove_before_second_newline(text):\n",
    "    lines = text.split('\\n')\n",
    "    new_text = '\\n'.join(lines[1:])\n",
    "    return new_text\n",
    "\n",
    "def extract_code(text):\n",
    "    pattern = r\"```([\\s\\S]*?)```\"\n",
    "    matches = re.findall(pattern, text)\n",
    "    code = []\n",
    "    for match in matches:\n",
    "        code.append(remove_before_second_newline(match))\n",
    "    return code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "languages = extract_languages(content)\n",
    "filenames = extract_filenames(content)\n",
    "code = extract_code(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['javascript', 'javascript', 'javascript']"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['app.js', 'app.test.js', 'corrected-app.js']"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"const app = express();\\n\\napp.get('/', (req, res) => {\\n  res.send('Hello World');\\n});\\n\\napp.listen(3000);\\n\",\n",
       " 'const app = require(\\'./app\\');\\n\\ndescribe(\\'GET /\\', () => {\\n  it(\\'responds with \"Hello World\"\\', (done) => {\\n    request(app)\\n      .get(\\'/\\')\\n      .expect(200, \\'Hello\\')\\n      .end(done);\\n  });\\n});\\n',\n",
       " \"const app = express();\\n\\napp.get('/', (req, res) => {\\n  res.send('Hello World');\\n});\\n\\napp.listen(3000);\\n\\nmodule.exports = app;\\n\"]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_checks():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    'labels': ['Incorrect code', 'Unit test', 'Corrected code'],\n",
    "    'languages': languages,\n",
    "    'filenames': filenames,\n",
    "    'code': code\n",
    "}\n",
    "\n",
    "json_filename = 'label_language_filename_code.json'\n",
    "with open(json_filename, 'w') as file:\n",
    "    json.dump(data, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RUN CELL TO SAVE CONTENT IN A TXT FILE\n",
    "import os\n",
    "\n",
    "folder_path = 'txt_outputs'\n",
    "text_files = [f for f in os.listdir(folder_path) if f.endswith('.txt')]\n",
    "txt_number_list = []\n",
    "for file_name in text_files:\n",
    "    txt_number = file_name[-5]\n",
    "    txt_number = int(txt_number)\n",
    "    txt_number_list.append(txt_number)\n",
    "if not txt_number_list: #check if list is empty, i.e. no txt files yet\n",
    "    new_txt_number = str(1)\n",
    "else:\n",
    "    new_txt_number = str(max(txt_number_list) + 1)\n",
    "text_file = open('txt_outputs/output' + '_' + new_txt_number + '.txt', 'w')\n",
    "text_file.write(system_message + '\\n')\n",
    "text_file.write(50*'-' + '\\n')\n",
    "text_file.write(user_message + '\\n')\n",
    "text_file.write(50*'-' + '\\n')\n",
    "text_file.write(content + '\\n')\n",
    "text_file.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
